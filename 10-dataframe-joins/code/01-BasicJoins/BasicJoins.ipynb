{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb366e5a-ca83-44fa-a9cb-717c0ae9987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/08 16:48:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.5.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark: SparkSession = (\n",
    "    SparkSession.builder.master(\"local[3]\")  # type: ignore\n",
    "    .appName(\"Basic Joins\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c50fe1a-2aa6-47b8-865b-6ee65320d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+---+\n",
      "|order_id|prod_id|unit_price|qty|\n",
      "+--------+-------+----------+---+\n",
      "|      01|     02|       350|  1|\n",
      "|      01|     04|       580|  1|\n",
      "|      01|     07|       320|  2|\n",
      "|      02|     03|       450|  1|\n",
      "|      02|     06|       220|  1|\n",
      "|      03|     01|       195|  1|\n",
      "|      04|     09|       270|  3|\n",
      "|      04|     08|       410|  2|\n",
      "|      05|     02|       350|  1|\n",
      "+--------+-------+----------+---+\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- unit_price: long (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersList = [\n",
    "    (\"01\", \"02\", 350, 1),\n",
    "    (\"01\", \"04\", 580, 1),\n",
    "    (\"01\", \"07\", 320, 2),\n",
    "    (\"02\", \"03\", 450, 1),\n",
    "    (\"02\", \"06\", 220, 1),\n",
    "    (\"03\", \"01\", 195, 1),\n",
    "    (\"04\", \"09\", 270, 3),\n",
    "    (\"04\", \"08\", 410, 2),\n",
    "    (\"05\", \"02\", 350, 1),\n",
    "]\n",
    "\n",
    "orderDF = spark.createDataFrame(ordersList).toDF(\n",
    "    \"order_id\",\n",
    "    \"prod_id\",\n",
    "    \"unit_price\",\n",
    "    \"qty\",\n",
    ")\n",
    "\n",
    "orderDF.show()\n",
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a339bf8-24a0-4ec9-9330-8b29436f8f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+---+\n",
      "|prod_id|          prod_name|list_price|qty|\n",
      "+-------+-------------------+----------+---+\n",
      "|     01|       Scroll Mouse|       250| 20|\n",
      "|     02|      Optical Mouse|       350| 20|\n",
      "|     03|     Wireless Mouse|       450| 50|\n",
      "|     04|  Wireless Keyboard|       580| 50|\n",
      "|     05|  Standard Keyboard|       360| 10|\n",
      "|     06|16 GB Flash Storage|       240|100|\n",
      "|     07|32 GB Flash Storage|       320| 50|\n",
      "|     08|64 GB Flash Storage|       430| 25|\n",
      "+-------+-------------------+----------+---+\n",
      "\n",
      "root\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- prod_name: string (nullable = true)\n",
      " |-- list_price: long (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productList = [\n",
    "    (\"01\", \"Scroll Mouse\", 250, 20),\n",
    "    (\"02\", \"Optical Mouse\", 350, 20),\n",
    "    (\"03\", \"Wireless Mouse\", 450, 50),\n",
    "    (\"04\", \"Wireless Keyboard\", 580, 50),\n",
    "    (\"05\", \"Standard Keyboard\", 360, 10),\n",
    "    (\"06\", \"16 GB Flash Storage\", 240, 100),\n",
    "    (\"07\", \"32 GB Flash Storage\", 320, 50),\n",
    "    (\"08\", \"64 GB Flash Storage\", 430, 25),\n",
    "]\n",
    "\n",
    "productDF = spark.createDataFrame(productList).toDF(\n",
    "    \"prod_id\",\n",
    "    \"prod_name\",\n",
    "    \"list_price\",\n",
    "    \"qty\",\n",
    ")\n",
    "\n",
    "productDF.show()\n",
    "productDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95f87d3-2bbe-4156-a1c5-d8cb0e48b93f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[AMBIGUOUS_REFERENCE] Reference `qty` is ambiguous, could be: [`qty`, `qty`].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m join_expr = orderDF.prod_id == productDF.prod_id\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# orderDF.join(productDF, join_expr, \"inner\").show()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43morderDF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproductDF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin_expr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morder_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprod_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munit_price\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m.show()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE]\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Reference `qty` is ambiguous, could be: [`qty`, `qty`].\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/spark-notes/.venv/lib/python3.13/site-packages/pyspark/sql/dataframe.py:3229\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   3184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   3185\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   3186\u001b[39m \n\u001b[32m   3187\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3227\u001b[39m \u001b[33;03m    +-----+---+\u001b[39;00m\n\u001b[32m   3228\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3229\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/spark-notes/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/spark-notes/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [AMBIGUOUS_REFERENCE] Reference `qty` is ambiguous, could be: [`qty`, `qty`]."
     ]
    }
   ],
   "source": [
    "join_expr = orderDF.prod_id == productDF.prod_id\n",
    "# orderDF.join(productDF, join_expr, \"inner\").show()\n",
    "\n",
    "orderDF.join(productDF, on=join_expr, how=\"inner\").select(\n",
    "    \"order_id\",\n",
    "    \"prod_name\",\n",
    "    \"unit_price\",\n",
    "    \"qty\",\n",
    ").show()\n",
    "\n",
    "# pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE]\n",
    "# Reference `qty` is ambiguous, could be: [`qty`, `qty`]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da5a02-4a5f-4380-abed-8e241aaa2e3b",
   "metadata": {},
   "source": [
    "## Handling Column Name Ambiguity in Spark Joins\n",
    "\n",
    "Column name ambiguity is a frequent challenge when joining DataFrames that contain columns with identical names (especially columns *not* used as join keys). Attempting to select or operate on such an ambiguous column in the resulting joined DataFrame will lead to an `AnalysisException`.\n",
    "\n",
    "**When Does It Typically Occur?**\n",
    "\n",
    "* When joining DataFrames where non-key columns share the same name (e.g., both DataFrames have a 'description' or 'value' column).\n",
    "* Even if the join key column has the same name (e.g., 'id') and you use `on=\"id\"` (which correctly results in only one 'id' column), other identically named columns remain ambiguous.\n",
    "* If using a Column expression like `left_df.id == right_df.id`, the resulting DataFrame might contain *both* 'id' columns (one from left, one from right), making them ambiguous unless handled.\n",
    "\n",
    "**Strategies to Prevent or Resolve Ambiguity:**\n",
    "\n",
    "1.  **Rename Columns Before Joining:** This is often the clearest and most recommended approach. Use `withColumnRenamed(\"old_name\", \"new_name\")` on one or both DataFrames *before* performing the join to ensure all potentially conflicting column names are unique.\n",
    "    ```python\n",
    "    # Example: Both DFs have a 'value' column, join on 'id'\n",
    "    right_renamed = right_df.withColumnRenamed(\"value\", \"right_value\")\n",
    "    joined_df = left_df.join(right_renamed, on=\"id\")\n",
    "    # Now 'value' (from left) and 'right_value' are distinct and usable\n",
    "    joined_df.select(\"id\", \"value\", \"right_value\").show()\n",
    "    ```\n",
    "\n",
    "2.  **Alias DataFrames Before Joining:** Use `df.alias(\"some_alias\")`. This allows you to reference columns unambiguously using the alias prefix (e.g., `col(\"alias.column_name\")`) both in the join condition and in subsequent operations like `select`.\n",
    "    ```python\n",
    "    l = left_df.alias(\"l\")\n",
    "    r = right_df.alias(\"r\")\n",
    "    joined_df = l.join(r, l.id == r.id)\n",
    "    # Select specific columns using aliases, renaming one 'value' column\n",
    "    joined_df.select(\n",
    "        col(\"l.id\"),\n",
    "        col(\"l.value\"),\n",
    "        col(\"r.value\").alias(\"right_value\")\n",
    "    ).show()\n",
    "    ```\n",
    "\n",
    "3.  **Select Specific Columns After Join (Using Aliases or Careful Referencing):** If you used aliases (Method 2), you can select using `col(\"alias.column_name\")`. If you joined without prior renaming or aliases and have ambiguous columns, selecting them directly (`joined_df.select(\"ambiguous_col\")`) will fail. You would typically need to have used Method 1 or 2 beforehand. While sometimes `joined_df[left_df.ambiguous_col]` might seem to work, relying on this can be fragile. The most robust methods involve explicit renaming or aliasing *before* the join.\n",
    "\n",
    "> Proactively renaming columns or using DataFrame aliases before joining is generally the safest and most readable way to handle potential column name ambiguities in Spark. Relying on selecting ambiguous columns after the join is often problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591c404f-a60b-40b2-9c74-f92290dbe68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+---+\n",
      "|order_id|          prod_name|unit_price|qty|\n",
      "+--------+-------------------+----------+---+\n",
      "|      03|       Scroll Mouse|       195|  1|\n",
      "|      01|      Optical Mouse|       350|  1|\n",
      "|      05|      Optical Mouse|       350|  1|\n",
      "|      02|     Wireless Mouse|       450|  1|\n",
      "|      01|  Wireless Keyboard|       580|  1|\n",
      "|      02|16 GB Flash Storage|       220|  1|\n",
      "|      01|32 GB Flash Storage|       320|  2|\n",
      "|      04|64 GB Flash Storage|       410|  2|\n",
      "+--------+-------------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.join(productDF, on=join_expr, how=\"inner\").select(\n",
    "    orderDF.order_id,\n",
    "    productDF.prod_name,\n",
    "    orderDF.unit_price,\n",
    "    orderDF.qty,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0554b703-b8a5-4ae6-b8d5-a17c471763ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+---+\n",
      "|order_id|          prod_name|unit_price|qty|\n",
      "+--------+-------------------+----------+---+\n",
      "|      03|       Scroll Mouse|       195|  1|\n",
      "|      01|      Optical Mouse|       350|  1|\n",
      "|      05|      Optical Mouse|       350|  1|\n",
      "|      02|     Wireless Mouse|       450|  1|\n",
      "|      01|  Wireless Keyboard|       580|  1|\n",
      "|      02|16 GB Flash Storage|       220|  1|\n",
      "|      01|32 GB Flash Storage|       320|  2|\n",
      "|      04|64 GB Flash Storage|       410|  2|\n",
      "+--------+-------------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productRenamedDF = productDF.withColumnRenamed(\"qty\", \"prod_qty\")\n",
    "\n",
    "# rename the column before joining to avoid ambiguity\n",
    "orderDF.join(productRenamedDF, on=join_expr, how=\"inner\").select(\n",
    "    \"order_id\",\n",
    "    \"prod_name\",\n",
    "    \"unit_price\",\n",
    "    \"qty\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2d95d58-445a-470d-b0ab-cbe270b062be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+----------+---+\n",
      "|order_id|prod_id|          prod_name|unit_price|qty|\n",
      "+--------+-------+-------------------+----------+---+\n",
      "|      03|     01|       Scroll Mouse|       195|  1|\n",
      "|      01|     02|      Optical Mouse|       350|  1|\n",
      "|      05|     02|      Optical Mouse|       350|  1|\n",
      "|      02|     03|     Wireless Mouse|       450|  1|\n",
      "|      01|     04|  Wireless Keyboard|       580|  1|\n",
      "|      02|     06|16 GB Flash Storage|       220|  1|\n",
      "|      01|     07|32 GB Flash Storage|       320|  2|\n",
      "|      04|     08|64 GB Flash Storage|       410|  2|\n",
      "+--------+-------+-------------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the column after joining to avoid ambiguity\n",
    "orderDF.join(productRenamedDF, on=join_expr, how=\"inner\").drop(\n",
    "    productRenamedDF.prod_id,\n",
    ").select(\n",
    "    \"order_id\",\n",
    "    \"prod_id\",\n",
    "    \"prod_name\",\n",
    "    \"unit_price\",\n",
    "    \"qty\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60179a80-cce3-490f-9ae6-36aa6a3e907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
