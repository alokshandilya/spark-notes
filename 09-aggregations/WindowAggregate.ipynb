{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd34c016-538b-4f82-83ac-36ad52bb5783",
   "metadata": {},
   "source": [
    "# What are Window Aggregations?\n",
    "\n",
    "Unlike standard aggregations (`groupBy().agg()`) which collapse rows into a single output row per group, window aggregations perform calculations across a set of rows (a \"window\") that are somehow related to the *current* row. The key benefit is that they **return a value for every input row** rather than collapsing them. This is useful for tasks like:\n",
    "\n",
    "- Calculating running totals or moving averages.\n",
    "- Ranking rows within groups (e.g., top N products per category).\n",
    "- Calculating differences between the current row and preceding/succeeding rows.\n",
    "\n",
    "## Core Components in PySpark\n",
    "\n",
    "1.  **Window Functions:** These are the functions you apply over the window (e.g., `sum`, `avg`, `rank`, `lag`). Many standard aggregate functions can be used as window functions. Specific window functions are available in `pyspark.sql.functions`.\n",
    "2.  **Window Specification (`WindowSpec`):** This defines the window (the set of rows) for the calculation. You create it using the `Window` class from `pyspark.sql.window`.\n",
    "\n",
    "**Defining the Window (`WindowSpec`)**\n",
    "\n",
    "A `WindowSpec` is defined primarily by three components:\n",
    "\n",
    "1.  **`partitionBy(*cols)`:**\n",
    "    * **Purpose:** Divides the rows of the DataFrame into independent partitions based on the specified column(s). The window function is applied separately within each partition.\n",
    "    * **Analogy:** Similar to `GROUP BY`.\n",
    "    * **Example:** `Window.partitionBy(\"department\")` - calculations will restart for each department.\n",
    "\n",
    "2.  **`orderBy(*cols)`:**\n",
    "    * **Purpose:** Orders the rows *within* each partition based on the specified column(s) and direction (ascending default, use `.desc()` for descending).\n",
    "    * **Importance:** Crucial for functions that depend on row order, like ranking (`rank`, `row_number`), offset (`lag`, `lead`), and cumulative calculations.\n",
    "    * **Example:** `Window.partitionBy(\"department\").orderBy(\"salary\")` - rows within each department are ordered by salary.\n",
    "\n",
    "3.  **Frame Definition (`rowsBetween(start, end)` / `rangeBetween(start, end)`)**\n",
    "    * **Purpose:** Specifies the exact set of rows relative to the current row *within the ordered partition* to include in the window frame for calculation.\n",
    "    * **Boundaries:** Defined relative to the `Window.currentRow`. Common boundaries include:\n",
    "        * `Window.unboundedPreceding`: The first row of the partition.\n",
    "        * `Window.unboundedFollowing`: The last row of the partition.\n",
    "        * `Window.currentRow`: The current row.\n",
    "        * Integer offsets (e.g., `-1`, `1`): Rows preceding or following the current row.\n",
    "    * **`rowsBetween(start, end)`:** Defines the frame based on a fixed number of rows relative to the current row (physical offset). Example: `rowsBetween(-1, 1)` includes the previous row, current row, and next row.\n",
    "    * **`rangeBetween(start, end)`:** Defines the frame based on a *value* range relative to the current row's value in the `orderBy` column. Requires ordering by only *one* column. All rows with values within the specified range are included. Example: `rangeBetween(Window.unboundedPreceding, Window.currentRow)` includes all rows from the start of the partition up to the current row's *value* (useful for cumulative sums where rows with the same value are treated together).\n",
    "    * **Default Frame:**\n",
    "        * If only `partitionBy` is used: The frame is the entire partition (`rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)`).\n",
    "        * If `orderBy` is used without specifying a frame: The default is usually `rangeBetween(Window.unboundedPreceding, Window.currentRow)`, suitable for cumulative calculations.\n",
    "\n",
    "## Common Window Functions (`pyspark.sql.functions`)\n",
    "\n",
    "* **Aggregate Functions:** `sum()`, `avg()`, `count()`, `min()`, `max()` applied over the window.\n",
    "* **Ranking Functions:**\n",
    "    * `rank()`: Assigns rank based on order; skips ranks after ties (e.g., 1, 1, 3).\n",
    "    * `dense_rank()`: Assigns rank based on order; does *not* skip ranks after ties (e.g., 1, 1, 2).\n",
    "    * `row_number()`: Assigns a unique, sequential number within the partition based on order, regardless of ties.\n",
    "    * `percent_rank()`: Rank as a percentage within the partition.\n",
    "    * `ntile(n)`: Divides rows into `n` ranked groups (buckets).\n",
    "* **Analytic (Offset) Functions:**\n",
    "    * `lag(col, offset=1, default=None)`: Gets the value of `col` from a previous row within the partition (defined by `offset`).\n",
    "    * `lead(col, offset=1, default=None)`: Gets the value of `col` from a subsequent row within the partition.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's calculate the salary rank for each employee within their department and the difference from the average department salary.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"WindowFunctionsExample\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (\"Sales\", \"Alice\", 5000), (\"Sales\", \"Bob\", 4500), (\"Sales\", \"Charlie\", 5500),\n",
    "    (\"HR\", \"David\", 3500), (\"HR\", \"Eve\", 4000),\n",
    "    (\"IT\", \"Frank\", 6000), (\"IT\", \"Grace\", 6500), (\"IT\", \"Heidi\", 6000)\n",
    "]\n",
    "columns = [\"department\", \"employee\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# --- Define Windows ---\n",
    "\n",
    "# Window partitioned by department, ordered by salary descending for ranking\n",
    "dept_salary_rank_window = Window.partitionBy(\"department\").orderBy(F.col(\"salary\").desc())\n",
    "\n",
    "# Window partitioned by department (no ordering needed for overall avg)\n",
    "dept_avg_window = Window.partitionBy(\"department\")\n",
    "\n",
    "# --- Apply Window Functions ---\n",
    "\n",
    "df_results = df.withColumn(\n",
    "    \"rank_in_dept\",\n",
    "    F.rank().over(dept_salary_rank_window) # Rank within department\n",
    ").withColumn(\n",
    "    \"dense_rank_in_dept\",\n",
    "    F.dense_rank().over(dept_salary_rank_window) # Dense Rank\n",
    ").withColumn(\n",
    "    \"avg_dept_salary\",\n",
    "    F.avg(\"salary\").over(dept_avg_window) # Avg salary for the whole department partition\n",
    ").withColumn(\n",
    "    \"diff_from_avg\",\n",
    "    F.col(\"salary\") - F.col(\"avg_dept_salary\") # Calculate difference from dept avg\n",
    ")\n",
    "\n",
    "# Example with Lag: Find previous employee's salary in the ranked list\n",
    "df_results = df_results.withColumn(\n",
    "    \"prev_emp_salary\",\n",
    "    F.lag(\"salary\", 1).over(dept_salary_rank_window) # Salary of the person ranked just above\n",
    ")\n",
    "\n",
    "# Example with Cumulative Sum\n",
    "dept_cumulative_window = Window.partitionBy(\"department\").orderBy(\"salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df_results = df_results.withColumn(\n",
    "    \"cumulative_salary\",\n",
    "    F.sum(\"salary\").over(dept_cumulative_window)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nDataFrame with Window Function Results:\")\n",
    "df_results.show()\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "> This example demonstrates how to define different window specifications and apply various functions (`rank`, `dense_rank`, `avg`, `lag`, `sum`) over those windows to add insightful columns without collapsing the original data structure. Remember that the choice of `partitionBy`, `orderBy`, and the frame definition (`rowsBetween`/`rangeBetween`) is critical to getting the desired calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b2b21-2552-4a47-b9ec-5dad4bb1674b",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "- we solved the previous exercise on grouping aggregates and got the following dataframe:\n",
    "\n",
    "Now, extend this summary to meet the following requirements:\n",
    "- compute week by week running total for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "939adb71-e508-4a3c-b5f0-b0eefee99151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/08 14:07:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/08 14:07:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.5.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "spark: SparkSession = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[3]\")  # type: ignore\n",
    "    .appName(\"WindowAggregate\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8dcac7-9181-4f8a-be5a-da7dc5eb283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+-------------+------------+\n",
      "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|\n",
      "+---------------+----------+-----------+-------------+------------+\n",
      "|      Australia|        48|          1|          107|      358.25|\n",
      "|      Australia|        49|          1|          214|       258.9|\n",
      "|      Australia|        50|          2|          133|      387.95|\n",
      "|        Austria|        50|          2|            3|      257.04|\n",
      "|        Bahrain|        51|          1|           54|      205.74|\n",
      "|        Belgium|        48|          1|          528|       346.1|\n",
      "|        Belgium|        50|          2|          285|      625.16|\n",
      "|        Belgium|        51|          2|          942|      838.65|\n",
      "|Channel Islands|        49|          1|           80|      363.53|\n",
      "|         Cyprus|        50|          1|          917|     1590.82|\n",
      "|        Denmark|        49|          1|          454|      1281.5|\n",
      "|           EIRE|        48|          7|         2822|     3147.23|\n",
      "|           EIRE|        49|          5|         1280|      3284.1|\n",
      "|           EIRE|        50|          5|         1184|     2321.78|\n",
      "|           EIRE|        51|          5|           95|      276.84|\n",
      "|        Finland|        50|          1|         1254|       892.8|\n",
      "|         France|        48|          4|         1299|     2808.16|\n",
      "|         France|        49|          9|         2303|     4527.01|\n",
      "|         France|        50|          6|          529|      537.32|\n",
      "|         France|        51|          5|          847|     1702.87|\n",
      "+---------------+----------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaryDF: DataFrame = spark.read.format(\"parquet\").load(\"output/*.parquet\")  # previous solution\n",
    "\n",
    "summarySortedDF = summaryDF.sort(\"Country\", \"WeekNumber\")\n",
    "\n",
    "summarySortedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d53fb3-4283-4266-a824-491df3b3a1da",
   "metadata": {},
   "source": [
    "1. **break dataframe by `Country`**\n",
    "2. _order each partition by the week number, for week by week total._\n",
    "3. compute total using sliding window of records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761acfcd-8ff6-4a8e-bb35-a551486f4d02",
   "metadata": {},
   "source": [
    "# 3 step process\n",
    "\n",
    "- Identify partitioning columns. (_here, `Country`_)\n",
    "- Identify ordering requirement. (_here, `WeekNumber`_)\n",
    "- Define Window start and end. (_here, first record is start, including everything till current record for a `country`_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbec3a9-0dcd-4adc-bf3a-54350790a28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+-------------+------------+\n",
      "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|\n",
      "+---------------+----------+-----------+-------------+------------+\n",
      "|      Australia|        48|          1|          107|      358.25|\n",
      "|      Australia|        49|          1|          214|       258.9|\n",
      "|      Australia|        50|          2|          133|      387.95|\n",
      "|        Austria|        50|          2|            3|      257.04|\n",
      "|        Bahrain|        51|          1|           54|      205.74|\n",
      "|        Belgium|        48|          1|          528|       346.1|\n",
      "|        Belgium|        50|          2|          285|      625.16|\n",
      "|        Belgium|        51|          2|          942|      838.65|\n",
      "|Channel Islands|        49|          1|           80|      363.53|\n",
      "|         Cyprus|        50|          1|          917|     1590.82|\n",
      "|        Denmark|        49|          1|          454|      1281.5|\n",
      "|           EIRE|        48|          7|         2822|     3147.23|\n",
      "|           EIRE|        49|          5|         1280|      3284.1|\n",
      "|           EIRE|        50|          5|         1184|     2321.78|\n",
      "|           EIRE|        51|          5|           95|      276.84|\n",
      "|        Finland|        50|          1|         1254|       892.8|\n",
      "|         France|        48|          4|         1299|     2808.16|\n",
      "|         France|        49|          9|         2303|     4527.01|\n",
      "|         France|        50|          6|          529|      537.32|\n",
      "|         France|        51|          5|          847|     1702.87|\n",
      "+---------------+----------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarySortedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8e85d5-e991-4082-a8d8-735e26e18582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+-------------+------------+------------+\n",
      "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|RunningTotal|\n",
      "+---------------+----------+-----------+-------------+------------+------------+\n",
      "|      Australia|        48|          1|          107|      358.25|      358.25|\n",
      "|      Australia|        49|          1|          214|       258.9|      617.15|\n",
      "|      Australia|        50|          2|          133|      387.95|      1005.1|\n",
      "|        Austria|        50|          2|            3|      257.04|      257.04|\n",
      "|        Bahrain|        51|          1|           54|      205.74|      205.74|\n",
      "|        Belgium|        48|          1|          528|       346.1|       346.1|\n",
      "|        Belgium|        50|          2|          285|      625.16|      971.26|\n",
      "|        Belgium|        51|          2|          942|      838.65|     1809.91|\n",
      "|Channel Islands|        49|          1|           80|      363.53|      363.53|\n",
      "|         Cyprus|        50|          1|          917|     1590.82|     1590.82|\n",
      "|        Denmark|        49|          1|          454|      1281.5|      1281.5|\n",
      "|           EIRE|        48|          7|         2822|     3147.23|     3147.23|\n",
      "|           EIRE|        49|          5|         1280|      3284.1|     6431.33|\n",
      "|           EIRE|        50|          5|         1184|     2321.78|     8753.11|\n",
      "|           EIRE|        51|          5|           95|      276.84|     9029.95|\n",
      "|        Finland|        50|          1|         1254|       892.8|       892.8|\n",
      "|         France|        48|          4|         1299|     2808.16|     2808.16|\n",
      "|         France|        49|          9|         2303|     4527.01|     7335.17|\n",
      "|         France|        50|          6|          529|      537.32|     7872.49|\n",
      "|         France|        51|          5|          847|     1702.87|     9575.36|\n",
      "+---------------+----------+-----------+-------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "# Window.unboundedPreceding: take all rows from beginning\n",
    "# runningWindowTotal = Window.partitionBy(\"Country\").orderBy(\"WeekNumber\").rowsBetween(-3, Window.currentRow)  # for 3 weeks\n",
    "runningWindowTotal = Window.partitionBy(\"Country\").orderBy(\"WeekNumber\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# summaryDF.withColumn(\"RunningTotal\", f.sum(\"InvoiceValue\").over(runningWindowTotal)).show()\n",
    "summaryDF.withColumn(\"RunningTotal\", f.round(f.sum(\"InvoiceValue\").over(runningWindowTotal), 2)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
